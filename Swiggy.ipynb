{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ede6b215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaning complete! Saved to: cleaned_swiggy.csv\n",
      "Rows before: 14998, after cleaning: 6760\n",
      "       id               name    city  rating  rating_count   cost  \\\n",
      "0  531342  Janta Sweet House  Abohar     4.4            50  200.0   \n",
      "1  158203  theka coffee desi  Abohar     3.8           100  100.0   \n",
      "2  187912          Singh Hut  Abohar     3.7            20  250.0   \n",
      "3  158204          Sam Uncle  Abohar     3.6            20  200.0   \n",
      "4  156588   shere punjab veg  Abohar     4.0           100  150.0   \n",
      "\n",
      "            cuisine    lic_no  \\\n",
      "0     Sweets,Bakery  1.21E+13   \n",
      "1         Beverages  2.21E+13   \n",
      "2  Fast Food,Indian  2.21E+13   \n",
      "3       Continental  2.21E+13   \n",
      "4      North Indian  2.21E+13   \n",
      "\n",
      "                                                link  \\\n",
      "0  https://www.swiggy.com/restaurants/janta-sweet...   \n",
      "1  https://www.swiggy.com/restaurants/theka-coffe...   \n",
      "2  https://www.swiggy.com/restaurants/singh-hut-n...   \n",
      "3  https://www.swiggy.com/restaurants/sam-uncle-c...   \n",
      "4  https://www.swiggy.com/restaurants/shere-punja...   \n",
      "\n",
      "                                             address              menu  \n",
      "0  Janta Sweet House, Bazar No.9, Circullar Road,...  Menu/531342.json  \n",
      "1         theka coffee desi, sahtiya sadan road city  Menu/158203.json  \n",
      "2    Singh Hut, CIRCULAR ROAD NEAR NEHRU PARK ABOHAR  Menu/187912.json  \n",
      "3  Sam Uncle, hanumangarh road near raja bajaj sh...  Menu/158204.json  \n",
      "4  shere punjab veg, major surinder chowk near ve...  Menu/156588.json  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def clean_swiggy_data(file_path, output_path=\"cleaned_swiggy.csv\"):\n",
    "    \"\"\"\n",
    "    Clean the Swiggy restaurant dataset:\n",
    "    1. Remove invalid ratings ('--', 'Too Few Ratings')\n",
    "    2. Convert rating to float\n",
    "    3. Clean and convert rating_count to int\n",
    "    4. Clean and convert cost to float\n",
    "    5. Fill missing license numbers\n",
    "    6. Trim and standardize text columns\n",
    "    7. Remove duplicates\n",
    "    8. Reset index\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # 1Ô∏è‚É£ Remove rows with invalid ratings\n",
    "    df_clean = df_clean[~df_clean['rating'].isin(['--', 'Too Few Ratings'])]\n",
    "\n",
    "    # 2Ô∏è‚É£ Convert rating to numeric (float)\n",
    "    df_clean['rating'] = pd.to_numeric(df_clean['rating'], errors='coerce')\n",
    "\n",
    "    # 3Ô∏è‚É£ Clean and convert rating_count\n",
    "    def clean_rating_count(val):\n",
    "        if isinstance(val, str):\n",
    "            val = re.sub(r'[^0-9]', '', val)\n",
    "            return int(val) if val.isdigit() else np.nan\n",
    "        return val\n",
    "\n",
    "    df_clean['rating_count'] = df_clean['rating_count'].apply(clean_rating_count)\n",
    "    df_clean = df_clean.dropna(subset=['rating_count'])\n",
    "\n",
    "    # 4Ô∏è‚É£ Clean and convert cost\n",
    "    df_clean['cost'] = (\n",
    "        df_clean['cost']\n",
    "        .astype(str)\n",
    "        .str.replace('‚Çπ', '', regex=False)\n",
    "        .str.replace(',', '', regex=False)\n",
    "        .str.strip()\n",
    "        .replace('nan', np.nan)\n",
    "    )\n",
    "    df_clean['cost'] = pd.to_numeric(df_clean['cost'], errors='coerce')\n",
    "\n",
    "    # 5Ô∏è‚É£ Handle missing license numbers\n",
    "    df_clean['lic_no'] = df_clean['lic_no'].fillna('Unknown')\n",
    "\n",
    "    # 6Ô∏è‚É£ Trim spaces and standardize text columns\n",
    "    text_cols = ['name', 'city', 'cuisine', 'address']\n",
    "    for col in text_cols:\n",
    "        df_clean[col] = df_clean[col].astype(str).str.strip()\n",
    "\n",
    "    # 7Ô∏è‚É£ Drop duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    # 8Ô∏è‚É£ Reset index\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    # ‚úÖ Save cleaned file\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Cleaning complete! Saved to: {output_path}\")\n",
    "    print(f\"Rows before: {len(df)}, after cleaning: {len(df_clean)}\")\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_df = clean_swiggy_data(\"swiggy.csv\")\n",
    "    print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd645e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msiva\\AppData\\Local\\Temp\\ipykernel_14164\\3483418784.py:45: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  encoded_data = encoded_data.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data Preprocessing Complete!\n",
      "Rows: 6760 | Columns: 5667\n",
      "Encoded data saved as: encoded_data.csv\n",
      "Encoder saved as: encoder.pkl\n",
      "Index alignment with cleaned data: True\n",
      "scikit-learn version: 1.7.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "import sklearn\n",
    "\n",
    "\n",
    "def preprocess_swiggy_data(cleaned_file, encoded_output=\"encoded_data.csv\", encoder_output=\"encoder.pkl\"):\n",
    "    \"\"\"\n",
    "    Data Preprocessing Pipeline for Swiggy Dataset\n",
    "\n",
    "    Steps:\n",
    "    1. Load cleaned data\n",
    "    2. Apply One-Hot Encoding to categorical features (name, city, cuisine)\n",
    "    3. Save encoder as pickle file (encoder.pkl)\n",
    "    4. Save preprocessed encoded dataset (encoded_data.csv)\n",
    "    5. Ensure indices match with cleaned_data.csv\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Load the cleaned dataset\n",
    "    cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "\n",
    "    # 2Ô∏è‚É£ Select categorical columns for encoding\n",
    "    categorical_cols = ['name', 'city', 'cuisine']\n",
    "\n",
    "    # 3Ô∏è‚É£ Initialize OneHotEncoder (version-safe)\n",
    "    try:\n",
    "        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')  # for sklearn >=1.4\n",
    "    except TypeError:\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')  # for older versions\n",
    "\n",
    "    # 4Ô∏è‚É£ Fit and transform categorical features\n",
    "    encoded_array = encoder.fit_transform(cleaned_data[categorical_cols])\n",
    "\n",
    "    # 5Ô∏è‚É£ Convert encoded data to DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "    # 6Ô∏è‚É£ Drop original categorical columns and merge encoded columns\n",
    "    encoded_data = pd.concat(\n",
    "        [cleaned_data.drop(columns=categorical_cols).reset_index(drop=True),\n",
    "         encoded_df.reset_index(drop=True)],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # 7Ô∏è‚É£ Ensure all features are numeric\n",
    "    encoded_data = encoded_data.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    # 8Ô∏è‚É£ Save the encoded data and encoder\n",
    "    encoded_data.to_csv(encoded_output, index=False)\n",
    "    with open(encoder_output, \"wb\") as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "    # 9Ô∏è‚É£ Verify index alignment\n",
    "    index_match = cleaned_data.reset_index(drop=True).index.equals(encoded_data.reset_index(drop=True).index)\n",
    "\n",
    "    # ‚úÖ Print summary\n",
    "    print(\"‚úÖ Data Preprocessing Complete!\")\n",
    "    print(f\"Rows: {len(encoded_data)} | Columns: {encoded_data.shape[1]}\")\n",
    "    print(f\"Encoded data saved as: {encoded_output}\")\n",
    "    print(f\"Encoder saved as: {encoder_output}\")\n",
    "    print(f\"Index alignment with cleaned data: {index_match}\")\n",
    "    print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "    return encoded_data, encoder\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_file = \"cleaned_swiggy.csv\"\n",
    "    encoded_data, encoder = preprocess_swiggy_data(cleaned_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ee3743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b961098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using 5663 numeric features for COSINE method.\n",
      "üîπ Using Cosine Similarity...\n",
      "\n",
      "üçΩÔ∏è Recommendations similar to 'Singh Hut':\n",
      "               name                    city             cuisine  rating   cost\n",
      "0  The Biryani Life           HSR,Bangalore  Biryani,Hyderabadi     3.8  250.0\n",
      "1  The Biryani Life   Indiranagar,Bangalore  Biryani,Hyderabadi     3.5  250.0\n",
      "2  The Biryani Life      JP Nagar,Bangalore  Biryani,Hyderabadi     3.5  250.0\n",
      "3  The Biryani Life  Geddalahalli,Bangalore  Biryani,Hyderabadi     3.2  250.0\n",
      "4  The Biryani Life   Mahadevpura,Bangalore  Biryani,Hyderabadi     3.3  250.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def restaurant_recommendation_system(cleaned_file, encoded_file, method=\"kmeans\", n_clusters=10):\n",
    "    \"\"\"\n",
    "    Build a Restaurant Recommendation System using either K-Means or Cosine Similarity.\n",
    "\n",
    "    Parameters:\n",
    "        cleaned_file: str -> path to cleaned_data.csv\n",
    "        encoded_file: str -> path to encoded_data.csv\n",
    "        method: str -> \"kmeans\" or \"cosine\"\n",
    "        n_clusters: int -> number of clusters for K-Means\n",
    "\n",
    "    Returns:\n",
    "        cleaned_data (with cluster labels if KMeans used)\n",
    "        recommend_function\n",
    "    \"\"\"\n",
    "\n",
    "    # Load datasets\n",
    "    cleaned_data = pd.read_csv('cleaned_data.csv')\n",
    "    encoded_data = pd.read_csv('encoded_data.csv')\n",
    "\n",
    "    # Ensure both datasets have aligned indices\n",
    "    cleaned_data = cleaned_data.reset_index(drop=True)\n",
    "    encoded_data = encoded_data.reset_index(drop=True)\n",
    "\n",
    "    # Keep only numeric columns\n",
    "    numeric_data = encoded_data.select_dtypes(include=[np.number])\n",
    "\n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    numeric_data_imputed = pd.DataFrame(imputer.fit_transform(numeric_data),\n",
    "                                        columns=numeric_data.columns)\n",
    "\n",
    "    print(f\"‚úÖ Using {numeric_data_imputed.shape[1]} numeric features for {method.upper()} method.\")\n",
    "\n",
    "    # ------------------ K-Means Method ------------------ #\n",
    "    if method.lower() == \"kmeans\":\n",
    "        print(\"üîπ Using K-Means Clustering...\")\n",
    "\n",
    "        # Apply K-Means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        cleaned_data[\"cluster\"] = kmeans.fit_predict(numeric_data_imputed)\n",
    "\n",
    "        print(f\"‚úÖ K-Means clustering completed ‚Äî created {n_clusters} clusters.\")\n",
    "\n",
    "        # Define recommendation function\n",
    "        def recommend_kmeans(restaurant_name, top_n=5):\n",
    "            if restaurant_name not in cleaned_data[\"name\"].values:\n",
    "                return f\"‚ùå Restaurant '{restaurant_name}' not found in dataset.\"\n",
    "\n",
    "            # Get cluster of the input restaurant\n",
    "            target_cluster = cleaned_data.loc[\n",
    "                cleaned_data[\"name\"] == restaurant_name, \"cluster\"\n",
    "            ].values[0]\n",
    "\n",
    "            # Get other restaurants in same cluster\n",
    "            cluster_members = cleaned_data[cleaned_data[\"cluster\"] == target_cluster]\n",
    "            recommendations = cluster_members[cluster_members[\"name\"] != restaurant_name].head(top_n)\n",
    "\n",
    "            return recommendations[[\"name\", \"city\", \"cuisine\", \"rating\", \"cost\"]]\n",
    "\n",
    "        return cleaned_data, recommend_kmeans\n",
    "\n",
    "    # ------------------ Cosine Similarity Method ------------------ #\n",
    "    elif method.lower() == \"cosine\":\n",
    "        print(\"üîπ Using Cosine Similarity...\")\n",
    "\n",
    "        # Compute cosine similarity matrix\n",
    "        similarity_matrix = cosine_similarity(numeric_data_imputed)\n",
    "\n",
    "        def recommend_cosine(restaurant_name, top_n=5):\n",
    "            if restaurant_name not in cleaned_data[\"name\"].values:\n",
    "                return f\"‚ùå Restaurant '{restaurant_name}' not found in dataset.\"\n",
    "\n",
    "            idx = cleaned_data[cleaned_data[\"name\"] == restaurant_name].index[0]\n",
    "            scores = list(enumerate(similarity_matrix[idx]))\n",
    "            scores = sorted(scores, key=lambda x: x[1], reverse=True)[1:top_n + 1]\n",
    "            indices = [i[0] for i in scores]\n",
    "\n",
    "            return cleaned_data.loc[indices, [\"name\", \"city\", \"cuisine\", \"rating\", \"cost\"]].reset_index(drop=True)\n",
    "\n",
    "        return cleaned_data, recommend_cosine\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose either 'kmeans' or 'cosine'.\")\n",
    "\n",
    "\n",
    "# ------------------ Example Usage ------------------ #\n",
    "if __name__ == \"__main__\":\n",
    "    cleaned_file = \"cleaned_data.csv\"\n",
    "    encoded_file = \"encoded_data.csv\"\n",
    "\n",
    "    # Choose method: \"kmeans\" or \"cosine\"\n",
    "    cleaned_data, recommend = restaurant_recommendation_system(cleaned_file, encoded_file, method=\"cosine\", n_clusters=10)\n",
    "\n",
    "    # Example recommendation\n",
    "    restaurant_name = \"Singh Hut\"\n",
    "    print(f\"\\nüçΩÔ∏è Recommendations similar to '{restaurant_name}':\")\n",
    "    print(recommend(restaurant_name, top_n=5))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
